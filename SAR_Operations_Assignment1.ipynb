{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "import pycocotools.coco as pyco\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "from torchvision.models import resnet18, resnet50, resnet101,\\\n",
    "    ResNet101_Weights, ResNet18_Weights, ResNet50_Weights\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SDSDataset(Dataset):\n",
    "    def __init__(self, root, annotation_file, resize):\n",
    "        self.root = root\n",
    "        self.coco = pyco.COCO(annotation_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.num_classes = len(self.coco.cats)\n",
    "        self.resize = resize\n",
    "        self.transform = Compose([\n",
    "            Resize(resize),\n",
    "            ToTensor()\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "\n",
    "        # Image ID of the input image\n",
    "        img_id = self.ids[index]\n",
    "        # Annotation IDs from coco\n",
    "        ann_ids = coco.getAnnIds(img_id)\n",
    "        # Load Annotation for the input image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # Get path for the input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        # Open input image\n",
    "        org_image = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        # Get size of input image\n",
    "        org_height = org_image.height\n",
    "        org_width = org_image.width\n",
    "\n",
    "        # Apply transformation (resize) to input image\n",
    "        image = self.transform(org_image)\n",
    "\n",
    "        # Get number of objects in the input image\n",
    "        num_objects = len(coco_annotation)\n",
    "\n",
    "        # Get bounding boxes and category labels\n",
    "        # Coco format: bbox = [xmin, ymin, width, height]\n",
    "        # Pytorch format: bbox = [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in range(num_objects):\n",
    "            # Convert and resize boxes\n",
    "            xmin = coco_annotation[i]['bbox'][0] / (org_width/self.resize[1])\n",
    "            ymin = coco_annotation[i]['bbox'][1] / (org_height/self.resize[0])\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2] / (org_width/self.resize[1])\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3] / (org_height/self.resize[0])\n",
    "            labels.append(coco_annotation[i]['category_id'])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        img_id = torch.tensor([img_id])\n",
    "\n",
    "        # Get (rectangular) size of bbox\n",
    "        areas = []\n",
    "        for i in range(num_objects):\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "\n",
    "        # Get Iscrowd\n",
    "        iscrowd = torch.zeros((num_objects,), dtype=torch.int64)\n",
    "\n",
    "        # Create annotation dictionary\n",
    "        annotation = dict()\n",
    "        annotation['boxes'] = boxes\n",
    "        annotation['labels'] = labels\n",
    "        annotation['image_id'] = img_id\n",
    "        annotation['area'] = areas\n",
    "        annotation['iscrowd'] = iscrowd\n",
    "\n",
    "        # Save width and height of the original image to rescale bounding boxes later on\n",
    "        annotation['org_h'] = torch.as_tensor(org_height, dtype=torch.int64)\n",
    "        annotation['org_w'] = torch.as_tensor(org_width, dtype=torch.int64)\n",
    "\n",
    "        return image, annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Images\n",
    "train_data_dir = '/kaggle/input/sds-dataset/compressed/images/train'\n",
    "test_data_dir = '/kaggle/input/sds-dataset/compressed/images/val'\n",
    "# Annotations\n",
    "train_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_train.json'\n",
    "test_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_val.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if Cuda is available\n",
    "print(f'Cuda available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    # If yes, use GPU\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    # If no, use CPU\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resize = (256, 256)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_dataset = SDSDataset(train_data_dir, train_annotation_dir, resize)\n",
    "test_dataset = SDSDataset(test_data_dir, test_annotation_dir, resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader_train = DataLoader(train_dataset,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True,\n",
    "                               collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Image and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image, annotations = train_dataset[1000]\n",
    "# Convert the image tensor to NumPy and permute it for visualization\n",
    "image_np = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image_np)\n",
    "\n",
    "# Get bounding boxes and labels from annotations\n",
    "boxes = annotations['boxes'].numpy()\n",
    "labels = annotations['labels'].numpy()\n",
    "\n",
    "# Loop through each bounding box and draw it\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = box\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.text(x1, y1, str(labels[i]), color='white')  # Optional: Add label text\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_filters_to_selected_images(folder_path, num_images=5):\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'png', 'jpeg'))][:num_images]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply filters\n",
    "        gaussian_filtered = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "        median_filtered = cv2.medianBlur(image, 5)\n",
    "        bilateral_filtered = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "        \n",
    "        # Display results\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "        axs[0, 0].imshow(image)\n",
    "        axs[0, 0].set_title(f\"Original Image: {image_file}\")\n",
    "        axs[0, 1].imshow(gaussian_filtered)\n",
    "        axs[0, 1].set_title(\"Gaussian Filter\")\n",
    "        axs[1, 0].imshow(median_filtered)\n",
    "        axs[1, 0].set_title(\"Median Filter\")\n",
    "        axs[1, 1].imshow(bilateral_filtered)\n",
    "        axs[1, 1].set_title(\"Bilateral Filter\")\n",
    "        for ax in axs.ravel(): ax.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def segment_and_evaluate(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    gray = rgb2gray(image)\n",
    "    \n",
    "    # K-Means Clustering\n",
    "    reshaped_image = image.reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(reshaped_image)\n",
    "    kmeans_segmented = kmeans.labels_.reshape(image.shape[:2])\n",
    "    \n",
    "    # Mean Shift Segmentation\n",
    "    mean_shift_segmented = sobel(gray)\n",
    "    \n",
    "    # Graph-Based Segmentation\n",
    "    graph_segmented = felzenszwalb(image, scale=100, sigma=0.5, min_size=50)\n",
    "    \n",
    "    # Boundary detection\n",
    "    edge_kmeans = canny(kmeans_segmented.astype(float))\n",
    "    edge_meanshift = canny(mean_shift_segmented)\n",
    "    edge_graph = canny(graph_segmented.astype(float))\n",
    "    \n",
    "    # Compute Jaccard Score for accuracy comparison\n",
    "    gt_edges = dilation(canny(gray), disk(1))\n",
    "    \n",
    "    kmeans_acc = jaccard_score(gt_edges.flatten(), edge_kmeans.flatten(), average='binary')\n",
    "    meanshift_acc = jaccard_score(gt_edges.flatten(), edge_meanshift.flatten(), average='binary')\n",
    "    graph_acc = jaccard_score(gt_edges.flatten(), edge_graph.flatten(), average='binary')\n",
    "    \n",
    "    # Display results\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set_title(\"Original Image\")\n",
    "    axs[0, 1].imshow(kmeans_segmented, cmap='gray')\n",
    "    axs[0, 1].set_title(f\"K-Means (Acc: {kmeans_acc:.3f})\")\n",
    "    axs[0, 2].imshow(mean_shift_segmented, cmap='gray')\n",
    "    axs[0, 2].set_title(f\"Mean Shift (Acc: {meanshift_acc:.3f})\")\n",
    "    axs[1, 0].imshow(graph_segmented, cmap='gray')\n",
    "    axs[1, 0].set_title(f\"Graph-Based (Acc: {graph_acc:.3f})\")\n",
    "    axs[1, 1].imshow(gt_edges, cmap='gray')\n",
    "    axs[1, 1].set_title(\"Ground Truth Edges\")\n",
    "    for ax in axs.ravel(): ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans_acc, meanshift_acc, graph_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_files = [os.path.join(train_data_dir, f) for f in os.listdir(train_data_dir) if f.endswith(('jpg', 'png', 'jpeg'))][:5]\n",
    "for image_file in image_files:\n",
    "    kmeans_acc, meanshift_acc, graph_acc = segment_and_evaluate(image_file)\n",
    "    print(f\"Image: {os.path.basename(image_file)}\\nK-Means Accuracy: {kmeans_acc:.3f}, Mean Shift Accuracy: {meanshift_acc:.3f}, Graph-Based Accuracy: {graph_acc:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Growing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Faster R-CNN Model\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn.eval().to(device)\n",
    "\n",
    "# Load Mask R-CNN Model\n",
    "mask_rcnn = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "mask_rcnn.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Region Growing Algorithm Implementation\n",
    "def region_growing(img, seed):\n",
    "    rows, cols = img.shape\n",
    "    segmented = np.zeros_like(img, dtype=np.uint8)\n",
    "    visited = np.zeros_like(img, dtype=np.bool_)\n",
    "    threshold = 10  # Intensity difference threshold\n",
    "    stack = [seed]\n",
    "\n",
    "    while stack:\n",
    "        x, y = stack.pop()\n",
    "        if visited[x, y]:\n",
    "            continue\n",
    "\n",
    "        visited[x, y] = True\n",
    "        segmented[x, y] = 255\n",
    "\n",
    "        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols and not visited[nx, ny]:\n",
    "                if abs(int(img[nx, ny]) - int(img[x, y])) < threshold:\n",
    "                    stack.append((nx, ny))\n",
    "\n",
    "    return segmented\n",
    "\n",
    "# Apply Region Growing with a seed point near the detected object\n",
    "seed_point = (200, 300)  # Example seed point, should be adapted based on input image\n",
    "region_growing_result = region_growing(gray, seed_point)\n",
    "\n",
    "with torch.no_grad():\n",
    "    detections = faster_rcnn([image_tensor])\n",
    "\n",
    "# Draw bounding boxes\n",
    "threshold = 0.5\n",
    "image_with_boxes = image_rgb.copy()\n",
    "for i, score in enumerate(detections[0]['scores']):\n",
    "    if score > threshold:\n",
    "        box = detections[0]['boxes'][i].cpu().numpy().astype(int)\n",
    "        cv2.rectangle(image_with_boxes, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "        \n",
    "# Display the region-growing result\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Region Growing Segmentation\")\n",
    "plt.imshow(region_growing_result, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connected Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to process the image in smaller patches to reduce memory load\n",
    "def process_in_patches(image, patch_size=256):\n",
    "    h, w = image.shape[:2]\n",
    "    processed_image = np.zeros_like(image, dtype=np.uint8)\n",
    "\n",
    "    for y in range(0, h, patch_size):\n",
    "        for x in range(0, w, patch_size):\n",
    "            # Extract patch\n",
    "            patch = image[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "            # Apply contour-based filtering on patch\n",
    "            contours, _ = cv2.findContours(patch, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            patch_mask = np.zeros_like(patch, dtype=np.uint8)\n",
    "\n",
    "            for contour in contours:\n",
    "                if cv2.contourArea(contour) > 500:  # Minimum threshold for objects\n",
    "                    cv2.drawContours(patch_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "            # Place processed patch back into full image\n",
    "            processed_image[y:y+patch_size, x:x+patch_size] = patch_mask\n",
    "\n",
    "    return processed_image\n",
    "\n",
    "# Process the region growing result in patches\n",
    "filtered_result_patches = process_in_patches(region_growing_result)\n",
    "\n",
    "# Display the final refined segmentation\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Refined Object Separation using Patch-based Processing\")\n",
    "plt.imshow(filtered_result_patches, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "modules = list(resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2]\n",
    "backbone = nn.Sequential(*modules)\n",
    "backbone.out_channels = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Anchor Generator\n",
    "anchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize FasterRCNN with Backbone and AnchorGenerator\n",
    "model = FasterRCNN(backbone=backbone,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   num_classes=train_dataset.num_classes)\n",
    "# Send model to device\n",
    "model.to(device)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "early_stopping_tolerance = 10\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define learning rate, optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = Adam(params, lr=learning_rate)\n",
    "lr_scheduler = StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start Training Process\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    for images, targets in data_loader_train:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        sum_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        sum_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader_test:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            average_loss = sum(loss for loss in loss_dict.values()) / batch_size\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_average_loss = average_loss\n",
    "\n",
    "    # If model improved, save weights\n",
    "    if best_average_loss >= average_loss:\n",
    "        best_average_loss = average_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            'model.pth'\n",
    "        )\n",
    "\n",
    "    # Otherwise, reduce learning rate\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    time_id = time.strftime('%Y_%m_%d-%H_%M')\n",
    "    print(f'[{time_id}] '\n",
    "          f'Epoch {epoch} of {num_epochs} - Loss: {average_loss} - LR: {str(lr_scheduler.get_last_lr()[0])} '\n",
    "          f'- Early Stopping: {early_stopping_counter}/{early_stopping_tolerance}')\n",
    "\n",
    "    if early_stopping_tolerance == early_stopping_counter:\n",
    "        break\n",
    "\n",
    "print('Training stopped')\n",
    "\n",
    "# if args.create_prediction_file:\n",
    "#     # Create prediction file in coco format:\n",
    "#     generate_prediction_file(model, data_loader_test, device, resize)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3753458,
     "sourceId": 6494315,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
